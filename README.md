# PartPrompt: Parse Trees Guided LLM Prompt Compression

This repository contains the code and data for the paper: [Parse Trees Guided LLM Prompt Compression](https://ieeexplore.ieee.org/document/11164467), accepted to **IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)**.

---

## üß† Overview

**PartPrompt** is a novel **selective prompt compression** method that leverages **linguistic parse trees** and **global hierarchical structure** to compress prompts for Large Language Models (LLMs). By transforming prompt compression into a **tree pruning problem**, PartPrompt effectively preserves key information while significantly reducing input length and computational cost.

### Why PartPrompt?

- üöÄ **Efficient Compression**: Reduces LLM inference cost and latency.
- üå≥ **Structure-Aware**: Incorporates syntactic and document-level structure.
- üß© **Coherence-Preserving**: Maintains prompt coherence and readability.
- üåç **Multi-Scenario Support**: Works across English, non-English, code, and long-context prompts.

## üî• Key Features

- **Parse Tree Guidance**: Uses syntactic parse trees to guide token selection.
- **Global Hierarchical Tree**: Models document structure (sentence ‚Üí paragraph ‚Üí section ‚Üí document).
- **Root-ward & Leaf-ward Propagation**: Adjusts token importance based on writing logic.
- **Tokenizer Alignment**: Avoids fragmented tokens and improves coherence.
- **Efficient Entropy Approximation**: Speeds up computation without sacrificing performance.

## üìä Performance Highlights

PartPrompt achieves **state-of-the-art performance** across:

- ‚úÖ **8 diverse datasets** (e.g., BBCnews, arXiv, PeopleDaily, CodeNet, HotpotQA, GSM8K, RULER, LongBench)
- ‚úÖ **Multiple compression ratios** (10%‚Äì90%)
- ‚úÖ **Various LLMs** (Mixtral, Llama, Qwen, etc.)
- ‚úÖ **Multiple metrics** (BLEU, ROUGE, BERTScore, discourse metrics)

---
## üöÄ Quick Start

To reproduce the experiments from the paper:

1. Install packages

```bash
pip install nltk transformers accelerate
```

2. Download [Llama-7B](https://huggingface.co/meta-llama/Llama-2-7b)  model from [HuggingFace](https://huggingface.co/)

3. Choose a dataset. For BBCnews, it's like this

```bash
python -u main.py BBCnews 0.2,0.3,0.5 1.0,1.0,0.7,4.0,0.0,100.0 1
```

- `0.2,0.3,0.5`: Compression ratios
- `1.0,1.0,0.7,4.0,0.0,100.0`: Hyperparameters


## Citation

If you find this repository helpful or use our method in your research, please consider citing our paper:

```
@article{mao2024partprompt,
  title={Parse Trees Guided LLM Prompt Compression},
  author={Mao, Wenhao and Hou, Chengbin and Zhang, Tianyu and Lin, Xinyu and Tang, Ke and Lv, Hairong},
  journal={arXiv preprint arXiv:2409.15395},
  year={2024}
}
```

## License

This project is licensed under the [MIT License](LICENSE).
